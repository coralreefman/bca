{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bb8fd-ad41-4074-8d2d-7b314c745237",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python Pillow scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b5a74-ca7b-499e-b092-4bf0611776fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O \"https://i.redd.it/1sme6te6knf01.jpg\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd12824-0aa9-430d-96ce-b1883cd1ffdf",
   "metadata": {},
   "source": [
    "Make a folder 'images' and move the image there, renaming it to car.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b013f1c-18d6-4134-991c-18365725893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir images && mv 1sme6te6knf01.jpg images/car.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20858a-e725-45a7-9f8e-78394f161bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = 'images/car.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da648c60-4ba4-4368-9768-ed700e574761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with open-cv\n",
    "import cv2\n",
    "# open-cv represents RGB images as BGR (reverse order)\n",
    "img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76994a21-416f-410a-99f6-b84524872928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with scikit\n",
    "from skimage import io\n",
    "\n",
    "img = io.imread(img_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45c1c7-a90a-4c3f-bab9-2ae3fbaa8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with PIL\n",
    "from PIL import Image\n",
    "\n",
    "img =  Image.open(img_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095673f-e0bc-45e9-9165-5b3ca7ff7ce2",
   "metadata": {},
   "source": [
    "When reading the image as a NumPy array `ndarray`, various image processing can be performed using NumPy functions.  \n",
    "\n",
    "By using an ndarray, you can get and set (change) pixel values, trim images, concatenate images, etc. Those who are familiar with NumPy can do various image processing without using libraries such as OpenCV.  \n",
    "\n",
    "Even when using OpenCV, OpenCV for Python treats image data as ndarray, so it is useful to know how to use NumPy (ndarray). In addition to OpenCV, many libraries such as scikit-image treat images as ndarray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca440e2-d825-49e6-adfd-7b51dfb1df69",
   "metadata": {},
   "source": [
    "#### Manipulating images with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb8c0f-c8cd-49e3-a2cf-46414c7ea978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = io.imread(img_path)\n",
    "\n",
    "# set rows 0 - 100 to zero\n",
    "image[:100] = 0\n",
    "\n",
    "# create a mask from all pixels that are 'darker' than 69\n",
    "mask = image < 69\n",
    "# color the mask white\n",
    "image[mask] = 255\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05328cc-3dc1-40ef-b85c-be71747583ef",
   "metadata": {},
   "source": [
    "### basic image processing with skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd2986-6a36-4e8e-999d-2fb0d78425fc",
   "metadata": {},
   "source": [
    "### threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0060a5-25ce-4b5c-a1ff-af855095f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage import filters\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "image = io.imread(img_path)\n",
    "# conversion to grayscale is neccessary for most skimage functions\n",
    "grayscale = rgb2gray(image)\n",
    "  \n",
    "# Setting the plot size\n",
    "plt.figure(figsize=(16, 8))\n",
    "  \n",
    "for i in range(10):\n",
    "    \n",
    "    # Iterating different thresholds\n",
    "    binarized_gray = (grayscale > i*0.1)*1\n",
    "    plt.subplot(2,5,i+1)\n",
    "    \n",
    "    # Rounding of the threshold\n",
    "    # value to 1 decimal point\n",
    "    plt.title(\"Threshold: >\"+str(round(i*0.1,1)))\n",
    "\n",
    "    # Displaying the binarized image\n",
    "    # of various thresholds\n",
    "    plt.imshow(binarized_gray, cmap = 'gray')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db87cb7-18f2-4bd8-887b-d795c0df5e82",
   "metadata": {},
   "source": [
    "#### Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a0bfe-c124-4633-95f4-3b201fb277ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import filters\n",
    "# from skimage.util import compare_images\n",
    "\n",
    "#load image and convert to grayscale for edge detection\n",
    "image = io.imread(img_path)\n",
    "grayscale = rgb2gray(image)\n",
    "\n",
    "# there are multiple filters to choose from\n",
    "edge_roberts = filters.roberts(grayscale)\n",
    "edge_sobel = filters.sobel(grayscale)\n",
    "\n",
    "# set up the plot for comparison\n",
    "fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True,\n",
    "                         figsize=(16, 16))\n",
    "\n",
    "axes[0].imshow(edge_roberts, cmap=plt.cm.gray)\n",
    "axes[0].set_title('Roberts Edge Detection')\n",
    "\n",
    "axes[1].imshow(edge_sobel, cmap=plt.cm.gray)\n",
    "axes[1].set_title('Sobel Edge Detection')\n",
    "\n",
    "# disable axis\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915e6e4-af84-4f3e-bb4f-e0b9d446b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "image = io.imread(img_path)\n",
    "grayscale = rgb2gray(image)\n",
    "\n",
    "# invert the edges\n",
    "image = rescale_intensity(1 - filters.sobel(grayscale))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a3481-f767-48ce-9fc5-7bcd4fee3db4",
   "metadata": {},
   "source": [
    "#### entropy (a measure of how \"random\" a probability distribution is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ad6a2-a3b4-4295-a20c-646835d361e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters, morphology, color\n",
    "\n",
    "entropy_width = 20\n",
    "\n",
    "image = io.imread(img_path)\n",
    "grayscale = rgb2gray(image)\n",
    "\n",
    "# calculate entropy\n",
    "entropy = filters.rank.entropy(grayscale, morphology.disk(entropy_width))\n",
    "\n",
    "# plot it\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.imshow(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0fffd-13f3-4f77-956a-269c986de01e",
   "metadata": {},
   "source": [
    "#### Feature Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260eef2b-59df-4412-98df-e3df0dfa90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import CENSURE\n",
    "\n",
    "image = io.imread(img_path)\n",
    "grayscale = rgb2gray(image)\n",
    "\n",
    "detector = CENSURE()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "detector.detect(grayscale)\n",
    "\n",
    "ax.imshow(image, alpha=0.5)\n",
    "ax.scatter(detector.keypoints[:, 1], detector.keypoints[:, 0],\n",
    "              2 ** detector.scales, facecolors='none', edgecolors='r')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd863760-3428-4222-bd54-72538de3c780",
   "metadata": {},
   "source": [
    "#### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ff729-96b6-4185-b3e2-7b50ec809527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required boundaries\n",
    "from skimage.segmentation import slic, mark_boundaries\n",
    "\n",
    "# Setting the plot figure\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "image = io.imread(img_path)\n",
    "grayscale = rgb2gray(image)\n",
    "\n",
    "# Applying SLIC segmentation\n",
    "# for the edges to be drawn over\n",
    "img_segments = slic(image, n_segments=50, compactness=1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Plotting the original image\n",
    "plt.imshow(image)\n",
    "\n",
    "# Detecting boundaries for labels\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Plotting the ouput of marked_boundaries\n",
    "# function i.e. the image with segmented boundaries\n",
    "plt.imshow(mark_boundaries(image, img_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc803cce-3b05-4bdb-ac47-248f5c3c8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import label2rgb\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.imshow(label2rgb(img_segments, image, kind = 'avg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0252b80-42ef-4eba-918a-2013118d874e",
   "metadata": {},
   "source": [
    "Semantic Segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c1cd1-4a7c-4cff-b6bf-b201a0cf5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(url='https://blogs.nvidia.com/wp-content/uploads/2019/10/Panoptic-Segmentation_Video-Trimmed.mp4', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fee040-28d8-421f-afe7-f27f6e042c86",
   "metadata": {},
   "source": [
    "#### Haar Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74888dca-6c89-436f-9e1b-aec072f2270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://thispersondoesnotexist.com/image --output images/person.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6ce88-0dfd-4641-8a18-cf0967b20270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.feature import Cascade\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "# Load the trained file from the module root.\n",
    "trained_file = data.lbp_frontal_face_cascade_filename()\n",
    "\n",
    "# Initialize the detector cascade.\n",
    "detector = Cascade(trained_file)\n",
    "\n",
    "image = io.imread(\"images/person.png\")\n",
    "\n",
    "detected = detector.detect_multi_scale(img=image,\n",
    "                                       scale_factor=1.2,\n",
    "                                       step_ratio=1,\n",
    "                                       min_size=(60, 60),\n",
    "                                       max_size=(1024, 1024))\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "img_desc = plt.gca()\n",
    "plt.set_cmap('gray')\n",
    "\n",
    "for patch in detected:\n",
    "\n",
    "    img_desc.add_patch(\n",
    "        patches.Rectangle(\n",
    "            (patch['c'], patch['r']),\n",
    "            patch['width'],\n",
    "            patch['height'],\n",
    "            fill=False,\n",
    "            color='r',\n",
    "            linewidth=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# ax.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e40a5-1e1d-4e2a-89b6-73d7af313a61",
   "metadata": {},
   "source": [
    "https://github.com/opencv/opencv/tree/master/data/haarcascades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78481475-b1e1-45f4-b43c-c6dc50d98e43",
   "metadata": {},
   "source": [
    "Let's plot the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2941f-264b-4fb2-97df-805e324937d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 5, figsize=(16, 16))\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "images = data.lfw_subset()\n",
    "\n",
    "for i in range(20):\n",
    "    ax[i].imshow(images[90+i], cmap=plt.cm.gray, interpolation='bicubic') # try with interpolation='bicubic'\n",
    "    ax[i].axis('off')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858143ea-f2a9-4e12-b24c-49ab7e1ca208",
   "metadata": {},
   "source": [
    "### OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4767d-56bf-4f1a-982b-c4b38711426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310417ec-a2af-44b6-924b-848428f35656",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ('https://github.com/ipython-books/'\n",
    "       'cookbook-2nd-data/blob/master/'\n",
    "       'family.zip?raw=true')\n",
    "r = io.BytesIO(requests.get(url).content)\n",
    "zipfile.ZipFile(r).extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0c691-899f-44a9-81cc-72c77136e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/person.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848d49d-9b05-42bc-88f4-21b8f4d6ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f777f-575b-4b46-b82b-fa0c7e4a16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5245a8-b873-4761-8d08-916fc6815074",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, w, h in face_cascade.detectMultiScale(\n",
    "        img, 1.3):\n",
    "    cv2.rectangle(\n",
    "        img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "rgbimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "ax.imshow(rgbimg, cmap=plt.cm.gray)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8c6ed-f712-44fe-828c-43186d32f817",
   "metadata": {},
   "source": [
    "### read image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb76c3b-ef10-4d9d-9849-7da608c470db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread(\"images/person.png\", cv.IMREAD_COLOR)\n",
    "# img = cv.cvtColor(cv.imread(\"images/person.png\"), cv.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a7acb-b985-49dd-b229-6f81b2205820",
   "metadata": {},
   "source": [
    "### resize and blend two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f17b3-bf05-44b0-b4cb-3b9b38709fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://thispersondoesnotexist.com/image --output images/person2.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f459346-544a-46c8-9daa-976f9c15dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img1 = cv2.imread('images/person.png')\n",
    "img2 = cv2.imread('images/person2.png')\n",
    "\n",
    "new_width = 1024\n",
    "new_height = 1024\n",
    "new_size = (new_width, new_height)\n",
    "\n",
    "img1_resized = cv2.resize(img1, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "img2_resized = cv2.resize(img2, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "weighted_sum = cv2.addWeighted(img1_resized, 0.5, img2_resized, 0.5,0)\n",
    "img = cv.cvtColor(weighted_sum, cv.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(weighted_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2f798-00c3-41f6-ac4c-20e55808ab18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### open a webcam stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16caab8f-a00c-41ff-b360-26384384cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from IPython.display import display, Image\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Description',\n",
    "    icon='square' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def view(button):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    i = 0\n",
    "    while True:\n",
    "        _, frame = cap.read()\n",
    "        \n",
    "        frame = cv2.flip(frame, 1) # if your camera reverses your image\n",
    "        # frame = cv.Canny(frame, 300, 600)\n",
    "        \n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        \n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "        \n",
    "        if stopButton.value==True:\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n",
    "            \n",
    "\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton,))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff8188-f0f5-47a6-a70a-4cf8dcb757a8",
   "metadata": {},
   "source": [
    "### haar cascades on webcam stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5502b-aae7-4b70-8682-ee1e1d11c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc4f5c-e736-498e-8a3e-742da587263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "\n",
    "detector = cv.CascadeClassifier(\"data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def view(button):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    i = 0\n",
    "    while True:\n",
    "        _, frame = cap.read()\n",
    "        \n",
    "        frame = cv2.flip(frame, 1) # if your camera reverses your image\n",
    "        # frame = cv.Canny(frame, 300, 600)\n",
    "        \n",
    "        frame = imutils.resize(frame, width=720)\n",
    "        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        results = detector.detectMultiScale( gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30), flags=cv.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "        for (fX, fY, fW, fH) in results:\n",
    "            # extract the face ROI\n",
    "            faceROI = gray[fY:fY + fH, fX:fX + fW]\n",
    "            cv.rectangle(frame, (fX, fY), (fX + fW, fY + fH), (0, 255, 0), 2)\n",
    "        \n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "             \n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "        \n",
    "        if stopButton.value==True:\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c47cd-3477-4bcf-ad90-432f45edc8a1",
   "metadata": {},
   "source": [
    "### Local example to replace face with blur or edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bdad5-759b-46b0-b940-c291363f5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import imutils\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "detector = cv.CascadeClassifier(\"haarcascades/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    frame = imutils.resize(frame, width=1024)\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    results = detector.detectMultiScale( gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30), flags=cv.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    for (fX, fY, fW, fH) in results:\n",
    "\t\t# extract the face ROI\n",
    "        faceROI = gray[fY:fY + fH, fX:fX + fW]\n",
    "\n",
    "        print(results)\n",
    "\n",
    "\t    # draw the face bounding box on the frame\n",
    "        cv.rectangle(frame, (fX, fY), (fX + fW, fY + fH), (0, 255, 0), 2)\n",
    "\n",
    "        # Canny edge detection.\n",
    "        edges = cv.Canny(faceROI, 100, 200)\n",
    "\n",
    "        blur = cv.blur(faceROI, (20, 20), 30)\n",
    "        print(frame[faceROI])\n",
    "\n",
    "        # Blur or Edges\n",
    "        # gray[fY:fY + fH, fX:fX + fW] = blur\n",
    "        gray[fY:fY + fH, fX:fX + fW] = edges\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv.imshow('frame', gray)\n",
    "\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
