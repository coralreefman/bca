{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe77b344",
   "metadata": {},
   "source": [
    "# Local Retrieval-Augmented Generation (RAG) Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95efa2",
   "metadata": {},
   "source": [
    "### What is Retrieval-Augmented Generation (RAG)?\n",
    "RAG = Combine retrieval (finding relevant information) with generation (producing responses).\n",
    "Useful for question-answering, creative text generation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299500f",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "First, let's focus on the retrieval part "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6dabf5",
   "metadata": {},
   "source": [
    "## Embeddings Basics\n",
    "\n",
    "We use sentence-transformer for our embeddings.  \n",
    "Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art embedding and reranker models.  \n",
    "You can check it out here:  \n",
    "https://www.sbert.net/index.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') # a small general purpose model (80MB)\n",
    "# this will download the model to your machine, might take a while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8d94f",
   "metadata": {},
   "source": [
    "Next, let's define some sentences and encode them.\n",
    "Encoding means that we convert our sentences into vectors (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Ping me if you need anything.\",\n",
    "    \"Let me know if you have any questions.\",\n",
    "    \"this email thread was used to train a drone\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b869d",
   "metadata": {},
   "source": [
    "When we print the shape of our embeddings we can see we have three sentences with 384 dimensions each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82050aa8",
   "metadata": {},
   "source": [
    "with our embeddings ready, we can now compute the semantic similarity between all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63ecbd",
   "metadata": {},
   "source": [
    "Let's visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c780bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(similarities, cmap=\"copper\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(sentences)))\n",
    "plt.yticks(range(len(sentences)))\n",
    "plt.title(\"Similarity Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca57dcba",
   "metadata": {},
   "source": [
    "Each row and column corresponds to one sentence. The number at position [i, j] shows similarity between sentence i and sentence j.\n",
    "As you can see sentence 0 and 1 are somwhat related but sentence 2 is not related to either of them. the diagonal line are the sentences being fully related to themselves.\n",
    "Sentence 0 (“ping me”) and Sentence 1 (“let me know”) → Similarity 0.435 (related).\n",
    "Sentence 0 and Sentence 2 (“train a drone”) → Similarity 0.1194 (unrelated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b6306",
   "metadata": {},
   "source": [
    "## Chunking (splitting text)\n",
    "\n",
    "Let's say we want to work with longer texts, it's a good idea to split or *chunk* it into smaller parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e23924",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"\"\"\n",
    "A measure of uncertainty of an outcome, rather than the perceived lack of order. \n",
    "A random sequence of events, symbols or steps often has no order and does not follow an intelligible pattern or combination. \n",
    "Randomness exists when some outcomes occur without any order, unpredictably, or by chance. \n",
    "These notions are distinct, but they all have a close connection to probability. \n",
    "Individual random events are unpredictable, but since they often follow a probability distribution, the frequency of different outcomes over numerous events (or “trials”) is predictable: \n",
    "when throwing two dice, the outcome of any particular roll is unpredictable, but a sum of 7 will occur twice as often as 4.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e4a56",
   "metadata": {},
   "source": [
    "We can write our own chunking functions, for example using a max character length per chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_length(text, max_length):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for word in text.split():\n",
    "        # Check if adding the next word exceeds the max length\n",
    "        if len(current_chunk) + len(word) + 1 <= max_length:\n",
    "            current_chunk += (word + \" \")\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = word + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk_text_by_length(example_text, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c12bee",
   "metadata": {},
   "source": [
    "as you can see that might not be the best approach as a lot of meaning is getting lost between chunks.\n",
    "Another approach could be to simply chunk by periods, so that we ideally end up with single sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_period(text):\n",
    "    sentences = text.split('.')\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            chunks.append(sentence + '.')\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk_text_by_period(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce0e9c",
   "metadata": {},
   "source": [
    "this is already an improvement, however it can result in chunks of varying length. Also it might be good in many cases to have bigger chunks, what about paragraphs for example? since text can have so many forms, there is no one-size-fits-all solution.\n",
    "There are some prewritten methods that you can use to chunk text that we will cover later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here add chunking functions\n",
    "\n",
    "chunks = chunk_text_by_period(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9422712",
   "metadata": {},
   "source": [
    "## Vector Databases (Chroma DB)\n",
    "\n",
    "We use Chroma as a vector database. When working with a large number of embeddings it's a good idea to store them in a DB. Chroma is an open-source search and retrieval database that you can easily deploy locally.  \n",
    "You can check it out here: https://www.trychroma.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d47cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create client and collection\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\"example_collection\")\n",
    "\n",
    "# Add embeddings manually\n",
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=chunks,\n",
    "    ids=[\"chunk1\", \"chunk2\", \"chunk3\", \"chunk4\", \"chunk5\"] # improve this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8cf7d5",
   "metadata": {},
   "source": [
    "## Similarity Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is random?\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps: Parsing PDF & Using LangChain (covered later)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
