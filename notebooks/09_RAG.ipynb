{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe77b344",
   "metadata": {},
   "source": [
    "# Local Retrieval-Augmented Generation (RAG) Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95efa2",
   "metadata": {},
   "source": [
    "### What is Retrieval-Augmented Generation (RAG)?\n",
    "RAG combines retrieval (finding relevant information) with generation (producing responses).\n",
    "This is useful for question-answering, creative text generation, etc.\n",
    "\n",
    "Think of it like giving an AI assistant access to a library. Instead of only relying on what it memorized during training, it can look up relevant information first, then give you a better answer.\n",
    "\n",
    "This is useful because:\n",
    "- LLMs have knowledge cutoffs and can't know everything\n",
    "- You can add your own documents/data to the system\n",
    "- Reduces hallucinations by grounding responses in actual sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299500f",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "First, let's focus on the retrieval part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6dabf5",
   "metadata": {},
   "source": [
    "## Embeddings Basics\n",
    "\n",
    "**What are embeddings?**  \n",
    "You can think of embeddings as a way to convert words, sentences, or documents into numbers that a computer can understand and compare (vectors in multi-dimensinal matrices). It's like giving each piece of text a unique fingerprint. Similar texts get similar fingerprint, different texts get different fingerprints. \n",
    "\n",
    "For example, \"Hello\" and \"Hi\" would have very similar embeddings because they mean similar things, while \"Hello\" and \"Banana\" would have very different embeddings.\n",
    "\n",
    "Once we have embeddings, we can do math with meaning! We can calculate how similar two pieces of text are by measuring the distance between their embedding vectors. This is the foundation of semantic search.  \n",
    "\n",
    "We use Sentence transformers for our embeddings.  \n",
    "Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art embedding and reranker models.  \n",
    "You can check it out here:  \n",
    "https://www.sbert.net/index.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embeddings_model = SentenceTransformer('all-MiniLM-L6-v2') # a small general purpose model (80MB)\n",
    "# this will download the model to your machine, might take a while when you run it for the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8d94f",
   "metadata": {},
   "source": [
    "Next, let's define some sentences and encode them.\n",
    "Encoding means that we convert our sentences into vectors (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Ping me if you need anything.\",\n",
    "    \"Let me know if you have any questions.\",\n",
    "    \"this email thread was used to train a drone\",\n",
    "]\n",
    "\n",
    "embedded_sentences = embeddings_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b869d",
   "metadata": {},
   "source": [
    "When we print the shape of our embeddings, we see that we have three sentences, each with 384 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedded_sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82050aa8",
   "metadata": {},
   "source": [
    "with our embeddings ready, we can now compute the semantic similarity between all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = embeddings_model.similarity(embedded_sentences, embedded_sentences)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63ecbd",
   "metadata": {},
   "source": [
    "Let's visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c780bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(similarities, cmap=\"copper\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(sentences)))\n",
    "plt.yticks(range(len(sentences)))\n",
    "plt.title(\"Similarity Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca57dcba",
   "metadata": {},
   "source": [
    "Each row and column corresponds to one sentence. The number at position [i, j] shows similarity between sentence i and sentence j.\n",
    "As you can see, sentences 0 and 1 are somewhat related, but sentence 2 is not related to either of them. The diagonal line shows sentences being fully related to themselves.\n",
    "Sentence 0 (“ping me”) and Sentence 1 (“let me know”) → Similarity 0.435 (related).\n",
    "Sentence 0 and Sentence 2 (“train a drone”) → Similarity 0.1194 (unrelated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b6306",
   "metadata": {},
   "source": [
    "## Chunking (splitting text)\n",
    "\n",
    "If we want to work with longer texts, it's a good idea to split, or *chunk*, them into smaller parts.\n",
    "\n",
    "Why chunk?\n",
    "- Embedding models have token limits (can't process infinite text)\n",
    "- Smaller chunks = more precise retrieval (you get the exact paragraph that answers your question)\n",
    "- Better performance when searching through large documents\n",
    "- LLMs work better with focused, relevant context rather than entire documents\n",
    "\n",
    "Chunking strategies to consider:\n",
    "- Sentence-based: Good for Q&A, preserves complete thoughts\n",
    "- Paragraph-based: Better for longer context, maintains topic coherence\n",
    "- Sliding window: Overlapping chunks to avoid losing context at boundaries\n",
    "- Semantic chunking: AI-powered chunking that understands topic boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e23924",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"\"\"\n",
    "A measure of uncertainty of an outcome, rather than the perceived lack of order. \n",
    "A random sequence of events, symbols or steps often has no order and does not follow an intelligible pattern or combination. \n",
    "Randomness exists when some outcomes occur without any order, unpredictably, or by chance. \n",
    "These notions are distinct, but they all have a close connection to probability. \n",
    "Individual random events are unpredictable, but since they often follow a probability distribution, the frequency of different outcomes over numerous events (or “trials”) is predictable: \n",
    "when throwing two dice, the outcome of any particular roll is unpredictable, but a sum of 7 will occur twice as often as 4.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e4a56",
   "metadata": {},
   "source": [
    "We can write our own chunking functions, for example, by using a maximum character length per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_length(text, max_length):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for word in text.split():\n",
    "        # Check if adding the next word exceeds the max length\n",
    "        if len(current_chunk) + len(word) + 1 <= max_length:\n",
    "            current_chunk += (word + \" \")\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = word + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk_text_by_length(example_text, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c12bee",
   "metadata": {},
   "source": [
    "As you can see, that might not be the best approach, as a lot of meaning is lost between chunks.  \n",
    "Another approach is to split the text at periods, so that each chunk ideally contains a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_period(text):\n",
    "    sentences = text.split('.')\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            chunks.append(sentence + '.')\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk_text_by_period(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce0e9c",
   "metadata": {},
   "source": [
    "This is already an improvement; however, it can result in chunks of varying length. In many cases, it might be better to have larger chunks, such as paragraphs. Since text can take many forms, there is no one-size-fits-all solution.\n",
    "There are some prewritten methods that you can use to chunk text that we will cover later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here add chunking functions\n",
    "\n",
    "chunks = chunk_text_by_period(example_text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9422712",
   "metadata": {},
   "source": [
    "## Vector Databases (Chroma DB)\n",
    "\n",
    "We use Chroma as a vector database. When working with a large number of embeddings, it's a good idea to store them in a database. Chroma is an open-source vector search and retrieval database that you can easily deploy locally.  \n",
    "You can check it out here: https://www.trychroma.com/  \n",
    "\n",
    "Why use a vector database?\n",
    "- Speed: Optimized for similarity search across millions of vectors\n",
    "- Persistence: Your embeddings are saved between sessions\n",
    "- Scalability: Can handle large document collections efficiently\n",
    "- Metadata: Store additional info with each chunk (source, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d47cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create client and collection\n",
    "client = chromadb.PersistentClient() # we use a persistent client to store our db \n",
    "# client.delete_collection(name=\"example_collection_rag\") # uncomment in case you want to start over\n",
    "collection = client.get_or_create_collection(\"example_collection_rag\")\n",
    "\n",
    "embedded_chunks = embeddings_model.encode(chunks)\n",
    "\n",
    "# generic ids for the chunks\n",
    "ids = [f\"chunk_{i+1}\" for i in range(len(chunks))]\n",
    "\n",
    "# Add embeddings manually\n",
    "collection.add(\n",
    "    embeddings=embedded_chunks,\n",
    "    documents=chunks,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8cf7d5",
   "metadata": {},
   "source": [
    "## Similarity Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is random?\"\n",
    "query_embedding = embeddings_model.encode(query)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd308713",
   "metadata": {},
   "source": [
    "## Ollama \n",
    "Ollama is a local runtime for working with LLMs on your local machine. We'll use it to simply download models and generate text based on our retrieved documents.  \n",
    "Essentially, it's a wrapper around https://github.com/ggml-org/llama.cpp but we'll stick with ollama for now since it's very easy to use.\n",
    "You can download Ollama here: https://ollama.com/download  \n",
    "\n",
    "Why Ollama?\n",
    "- Privacy: Everything runs locally, no data sent to external APIs\n",
    "- Cost: Free to use once you have the hardware\n",
    "- Speed: No network latency for inference\n",
    "- Customization: Full control over model parameters  \n",
    "\n",
    "How it works:  \n",
    "\n",
    "You can serve ollama after you installed it by typing `ollama serve` in your terminal. Then you can use a second terminal window to interact with it.\n",
    "\n",
    "If you run the GUI, it automatically serves all downloaded models on your PC.\n",
    "\n",
    "You can check which models you already downloaded with `ollama list`  \n",
    "\n",
    "To run a model from your terminal, simply `ollama run` and select a model for example `ollama run gemma3:1b`  if you haven't downloaded it yet it will do so automatically.  \n",
    "\n",
    "to stop chatting with a model, type `/bye`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925e1a9",
   "metadata": {},
   "source": [
    "To use ollama in python, we need to `pip install ollama` to install the bindings.  \n",
    "Then we can test it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "# the query we send to the LLM\n",
    "query = \"What is randomness?\"\n",
    "\n",
    "# this is the structure expected by the ollama API\n",
    "response: ChatResponse = chat(model='gemma3:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': query,\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa5faa",
   "metadata": {},
   "source": [
    "## Connecting Retrieval and Generation\n",
    "\n",
    "This is where RAG comes together! \n",
    " \n",
    "- **R**etrieve relevant chunks based on the user's question\n",
    "- **A**ugment the prompt by including this context\n",
    "- **G**enerate a response using both the context and the LLM's knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query, top_k=1):\n",
    "    \n",
    "    # RETRIEVE\n",
    "    query_embedding = embeddings_model.encode(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    context = \"\\n\".join(results['documents'][0])\n",
    "    \n",
    "    # AUGMENT\n",
    "    # Prompt the local Ollama model\n",
    "    prompt = f\"Using the following context, answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\\n\"\n",
    "\n",
    "    print(f\"Prompt:\\n{prompt}\")\n",
    "    \n",
    "    # GENERATE\n",
    "    response: ChatResponse = chat(\n",
    "        model='gemma3:1b', \n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ]\n",
    "        )\n",
    "    return response.message.content\n",
    "\n",
    "# Test query\n",
    "query = \"What is random?\"\n",
    "response = query_rag(query)\n",
    "\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ee00f",
   "metadata": {},
   "source": [
    "## Parsing PDF's "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b365d08",
   "metadata": {},
   "source": [
    "### PyMuPDF & Tesseract\n",
    "\n",
    "PyMuPDF is a simple library for parsing PDF's.  \n",
    "To perform Optical Character Recognition (OCR) on scanned files or images of text, we'll also install Tesseract.   \n",
    "The results may vary, as these libraries are fast and easy to install, but not always perfect at text extraction.\n",
    "\n",
    "You can see how to install tesseract on your OS here:  \n",
    "https://tesseract-ocr.github.io/tessdoc/Installation.html  \n",
    "https://github.com/UB-Mannheim/tesseract/wiki  \n",
    "\n",
    "To install PyMuPDF: \n",
    "`pip install pymupdf`  \n",
    "\n",
    "https://github.com/pymupdf/PyMuPDF  \n",
    "https://github.com/tesseract-ocr/tesseract  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "doc = pymupdf.open(\"/Users/c/Desktop/pdf_extraction/PDF/138641610-Ways-of-Seeing.pdf\") # open a document\n",
    "\n",
    "text_pages = []\n",
    "\n",
    "for page in doc: # iterate the document pages\n",
    "  text = page.get_text() # get plain text encoded as UTF-8\n",
    "  text_pages.append(text)\n",
    "\n",
    "text_all = \"\\n\".join(text_pages)\n",
    "\n",
    "print(text_all)\n",
    "print(len(text_all))\n",
    "\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d213ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_pages[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11243309",
   "metadata": {},
   "source": [
    "Chances are your computer already performed OCR on your PDF or the text is actually embedded in your PDF in which case the text extraction should happen rather quickly.  \n",
    "In case it's not, or when you want to run OCR yourself PyMuPDF will use tesseract for OCR. There are also other LLM-assisted libraries that will perform better at this task but these usually require a decent GPU to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(\"/Users/c/Desktop/pdf_extraction/PDF/138641610-Ways-of-Seeing.pdf\")\n",
    "\n",
    "# force OCR for the first 5 pages\n",
    "for i in range(min(5, doc.page_count)):\n",
    "    page = doc[i]\n",
    "    tp = page.get_textpage_ocr(dpi=400, language=\"eng\", full=True)\n",
    "    txt = tp.extractText(\"text\")\n",
    "    print(f\"\\n--- page {i+1} OCR text ---\\n\", txt[:500])\n",
    "\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee9723",
   "metadata": {},
   "source": [
    "There is an additional package called pymupdf4llm that we can use to convert our pdf to markdown, which is much better suited for RAG than regular text files, as we can use the structure for chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea526244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "md_text = pymupdf4llm.to_markdown(\"/Users/c/Desktop/pdf_extraction/PDF/138641610-Ways-of-Seeing.pdf\")\n",
    "\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now work with the markdown text, e.g. store as a UTF8-encoded file\n",
    "import pathlib\n",
    "pathlib.Path(\"/Users/c/Desktop/pdf_extraction/output.md\").write_bytes(md_text.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495e304",
   "metadata": {},
   "source": [
    "### Docling\n",
    "Docling is a document parser with local AI models for OCR.\n",
    "It will perform vastly better than our approach above, but depending on your machine it will also take significantly longer.  \n",
    "It is still a good solution for local parsing and it also converts text to various formats and even chunks it for LLM use.\n",
    "The Docling AI models are relatively small and should work fine on a rather recent macbook or similar.  \n",
    "This will download several models for detection and recognition to your PC, so make sure you have some space left on your hard drive. \n",
    "The Docling project was started by the AI for knowledge team at IBM Research Zurich.  \n",
    "\n",
    "`pip install docling`  \n",
    "\n",
    "https://github.com/docling-project/docling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"/Users/c/Desktop/pdf_extraction/PDF/138641610-Ways-of-Seeing.pdf\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())\n",
    "result.save_as_markdown(\"/Users/c/Desktop/pdf_extraction/out-md.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6997f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerated workflow for macs\n",
    "# pip install mlx-vlm\n",
    "\n",
    "from docling.datamodel import vlm_model_specs\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import VlmPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "source = \"/Users/c/Desktop/pdf_extraction/PDF/138641610-Ways-of-Seeing.pdf\"  # document per local path or URL\n",
    "\n",
    "pipeline_options = VlmPipelineOptions(\n",
    "    vlm_options=vlm_model_specs.SMOLDOCLING_MLX,\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_cls=VlmPipeline,\n",
    "            pipeline_options=pipeline_options,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "doc = converter.convert(source=source).document\n",
    "print(doc.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.save_as_markdown(\"/Users/c/Desktop/pdf_extraction/out-markdown.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbdea4b",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "We use docling for chunking, however there are many other libraries, such as LangChain or Sentence Transformers that could also do this.  \n",
    "There are various approaches to chunking and the best approach depends on your data and LLM pipeline.  \n",
    "The docling HybridChunker is pretty versatile and since we already installed the package, let's use it to chunk our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "DOC_SOURCE = \"/Users/c/Desktop/pdf_extraction/out-markdown.md\"\n",
    "\n",
    "doc = DocumentConverter().convert(source=DOC_SOURCE).document\n",
    "\n",
    "chunker = HybridChunker()\n",
    "chunk_iter = chunker.chunk(dl_doc=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunk_iter):\n",
    "    print(f\"=== {i} ===\")\n",
    "    print(f\"chunk.text:\\n{f'{chunk.text[:300]}…'!r}\")\n",
    "\n",
    "    enriched_text = chunker.contextualize(chunk=chunk)\n",
    "    print(f\"chunker.contextualize(chunk):\\n{f'{enriched_text[:300]}…'!r}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6653c",
   "metadata": {},
   "source": [
    "### Embedding the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4feeea",
   "metadata": {},
   "source": [
    "We use the same chroma setup as above and use it to encode our chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create client and collection\n",
    "client = chromadb.PersistentClient() # we use a persistent client to store our db \n",
    "# client.delete_collection(name=\"example_collection\")\n",
    "collection = client.create_collection(\"example_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87241e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_SOURCE = \"/Users/c/Desktop/pdf_extraction/out-markdown.md\"\n",
    "\n",
    "doc = DocumentConverter().convert(source=DOC_SOURCE).document\n",
    "\n",
    "chunker = HybridChunker()\n",
    "chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "\n",
    "    enriched_text = chunker.contextualize(chunk=chunk)\n",
    "    \n",
    "    embeddings = embeddings_model.encode(enriched_text)\n",
    "\n",
    "    # Optionally, add metadata\n",
    "    metadata = {\"chunk_id\": i}\n",
    "\n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        documents=enriched_text,\n",
    "        metadatas=metadata,\n",
    "        ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Added chunk {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c59abf",
   "metadata": {},
   "source": [
    "Now we can use the same approach as above to query our LLM and augment it with our retrieved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "# make sure to 'ollama serve'\n",
    "\n",
    "def query_rag(query, top_k=4):\n",
    "    \n",
    "    query_embedding = embeddings_model.encode(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    context = \"\\n\".join(results['documents'][0])\n",
    "    \n",
    "    # Prompt the local Ollama model\n",
    "    prompt = f\"Using the following context, answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\\n\"\n",
    "\n",
    "    print(f\"Prompt:\\n{prompt}\")\n",
    "    \n",
    "    response: ChatResponse = chat(\n",
    "        model='gemma3:1b', \n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ]\n",
    "        )\n",
    "    return response.message.content\n",
    "\n",
    "# Test query\n",
    "query = \"how to look at art?\"\n",
    "response = query_rag(query)\n",
    "\n",
    "print(f\"Response:\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
